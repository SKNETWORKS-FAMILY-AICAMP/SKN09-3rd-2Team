{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2UFCcMjq2Dc"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfTZyt3cs_zX"
      },
      "outputs": [],
      "source": [
        "!pip install typing_extensions pydantic openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXAq8u8RtBpP"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers peft trl bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i7xOy99tj-7"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsWfUxYQtGBL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "import bitsandbytes as bnb\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5YC0nBBtISi"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCOWkaFtvS4"
      },
      "outputs": [],
      "source": [
        "# base 모델 담는 객체\n",
        "model_name = \"google/gemma-3-4b-it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCmamY2Lt3Az"
      },
      "outputs": [],
      "source": [
        "bnb_config = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    \"device_map\": \"auto\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl0cwfzHt65g"
      },
      "outputs": [],
      "source": [
        "# 토크나이저 및 모델 로드 (모델 로드 시 4-bit 양자화 설정)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, **bnb_config)\n",
        "# base model 작업 끝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwZgjXhqX-yo"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bvOpIfW2-he"
      },
      "source": [
        "---\n",
        "### 학습용 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkmbjoaPu996"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리 함수\n",
        "def preprocess_data(examples):\n",
        "\n",
        "    # 입력 데이터 (질문 + 문맥)\n",
        "    inputs = [q,'답변:',a for q,a in zip(examples[\"question\"], examples[\"answers\"])]\n",
        "    # 정답 데이터 추출\n",
        "    answer_texts = [for a in examples[\"answers\"]]\n",
        "\n",
        "    # 입력 데이터 토큰화\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # 정답 데이터 토큰화\n",
        "    labels = tokenizer(\n",
        "        answer_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    # input_ids 기준으로 labels 길이 맞춤\n",
        "    max_length = model_inputs[\"input_ids\"].shape[1]\n",
        "    labels = labels[:, :max_length]\n",
        "\n",
        "    # 패딩된 부분을 -100으로 설정 (loss 계산에서 무시)\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KBsfe8wXdsG"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 전처리 적용\n",
        "train_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset.column_names)\n",
        "val_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agd95hMy23PT"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d9F0SuV28Tu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLNo_J0m2TrQ"
      },
      "outputs": [],
      "source": [
        "# LoRA 설정\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# LoRA 적용\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.enable_input_require_grads()\n",
        "model.gradient_checkpointing_enable()\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3kfUiA82V2Q"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./q_lora_korqa',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOxleICo3Whn"
      },
      "source": [
        "# 주석에 데이터를 추가해야함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWnf2_o33WG_"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyodFlH83h0y"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT8NP_bA3q73"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR7-T0Re3tpX"
      },
      "source": [
        "학습후 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPZk2oEx3qfV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "trained_model_path = \"./q_lora_korqa/checkpoint-5661\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.save_pretrained(trained_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbYL_C8i3xvV"
      },
      "outputs": [],
      "source": [
        "adapter_model_path = \"./q_lora_korqa/checkpoint-5661\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_model_path)\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA2ndSjF30mq"
      },
      "outputs": [],
      "source": [
        "question = \"\"\n",
        "context = \"\"\n",
        "\n",
        "input_text = f\"질문: {question}\\n문맥: {context}\\n답변:\"\n",
        "\n",
        "output = qa_pipeline(input_text, max_new_tokens=50, temperature=0.7, top_p=0.8)\n",
        "\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
