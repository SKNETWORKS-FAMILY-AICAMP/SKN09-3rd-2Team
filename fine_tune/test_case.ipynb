{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=false\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install typing_extensions pydantic openai\n",
    "!pip install datasets transformers peft trl bitsandbytes\n",
    "!pip install sentence-transformers langchain langchain_community\n",
    "!pip install chromadb\n",
    "#!pip uninstall numpy -y\n",
    "# !pip install numpy==1.26.4\n",
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langsmith import Client\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline 래핑\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# 임베딩 및 벡터스토어 로드\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"chroma_index\",\n",
    "    embedding_function=embedding\n",
    ")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "# ✅ 사용자 정의 프롬프트\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "        너는 항공기 반입 금지 및 제한 물품에 대하여 친절하게 답해주고 나라별 여행지 추천 및 문화차이를 잘 설명할 수 있어\n",
    "        문맥에 해당 하는 내용들만 신뢰도있게 대답해야해\n",
    "\n",
    "        ### 질문:\n",
    "        {question}\n",
    "        \n",
    "        ### 문맥:\n",
    "        {context}\n",
    "\n",
    "        ### 답변:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# JSON 파일 로드\n",
    "with open(\"test_case.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_cases = test_data[\"test_cases\"]\n",
    "\n",
    "# RetrievalQA 체인 구성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 테스트 실행\n",
    "for test in test_cases:\n",
    "    print(f\"\\n[테스트 ID: {test['id']}]\")\n",
    "    print(f\"질문: {test['question']}\")\n",
    "    expected = test[\"expected_answer\"]\n",
    "    \n",
    "    # 모델 응답 얻기\n",
    "    result = qa_chain.run(test[\"question\"])\n",
    "    \n",
    "    print(f\"✅ 기대 응답: {expected}\")\n",
    "    print(f\"🤖 모델 응답: {result[\"result\"].split(\"### 답변:\\n\")[-1].strip()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
